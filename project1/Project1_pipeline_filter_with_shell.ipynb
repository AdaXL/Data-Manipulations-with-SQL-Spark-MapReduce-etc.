{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Word counts (40 points)\n",
    "\n",
    "## Part A. Characters in The Hound of the Baskervilles\n",
    "\n",
    "Use the text available at https://s3.amazonaws.com/2018-dmfa/project-1/hound.txt for this part.\n",
    "\n",
    "How many times are each of the following characters mentioned by name in the text of The Hound of the Baskervilles?\n",
    "\n",
    "* Holmes, Watson, Barrymore, Mortimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:55:11--  https://s3.amazonaws.com/2018-dmfa/project-1/hound.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.18.163\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.18.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 345659 (338K) [text/plain]\n",
      "Saving to: ‘hound.txt’\n",
      "\n",
      "hound.txt           100%[===================>] 337.56K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2018-09-24 01:55:11 (57.9 MB/s) - ‘hound.txt’ saved [345659/345659]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/project-1/hound.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take a peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s The Hound of the Baskervilles, by A. Conan Doyle\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: The Hound of the Baskervilles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head hound.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We exclude cases like \"Barrymores\" with an \"s\", since it's not mentioned as the character's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "113\n",
      "72\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "!grep -wc -i Holmes hound.txt\n",
    "!grep -wc -i Watson hound.txt\n",
    "!grep -wc -i Barrymore hound.txt\n",
    "!grep -wc -i Mortimer hound.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Holmes is mentioned by name 192 times in the text;\n",
    "- Watson is mentioned by name 113 times in the text;\n",
    "- Barrymore is mentioned by name 72 times in the text;\n",
    "- Mortimer is mentioned by name 89 times in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Characters in Hamlet\n",
    "\n",
    "Use the text available at https://s3.amazonaws.com/2018-dmfa/project-1/hamlet.txt for this part.\n",
    "\n",
    "How many times do each of the characters Hamlet, Polonius, Ophelia, and Horatio have speaking lines in Hamlet? Keep in mind that this is the text of a play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:55:41--  https://s3.amazonaws.com/2018-dmfa/project-1/hamlet.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.232.37\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.232.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204885 (200K) [text/plain]\n",
      "Saving to: ‘hamlet.txt’\n",
      "\n",
      "hamlet.txt          100%[===================>] 200.08K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2018-09-24 01:55:41 (33.2 MB/s) - ‘hamlet.txt’ saved [204885/204885]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/project-1/hamlet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When character speaks, there are 2 cases:\n",
    "- case 1: the name is capitalised and followed by a dot\n",
    "- case 2: 2 charactor speak together, for example, \"HORATIO and MARCELLUS.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n",
      "86\n",
      "58\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "!cat hamlet.txt | grep -e ^HAMLET'[[:space:]]' -e ^HAMLET'[.]' | wc -l\n",
    "!cat hamlet.txt | grep -e ^POLONIUS'[[:space:]]' -e ^POLONIUS'[.]' | wc -l\n",
    "!cat hamlet.txt | grep -e ^OPHELIA'[[:space:]]' -e ^OPHELIA'[.]' | wc -l\n",
    "!cat hamlet.txt | grep -e ^HORATIO'[[:space:]]' -e ^HORATIO'[.]' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hamlet has 358 speaking lines in Hamlet;\n",
    "- Polonius has 86 speaking lines in Hamlet;\n",
    "- Ophelia has 58 speaking lines in Hamlet;\n",
    "- Horatio has 110 speaking lines in Hamlet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Capital Bikeshare (40 points)\n",
    "\n",
    "Use the data available at https://s3.amazonaws.com/2018-dmfa/project-1/2018-capitalbikeshare-tripdata.zip for this problem.\n",
    "\n",
    "## Part A (20 points)\n",
    "\n",
    "1. Unzip the data and combine the two CSV files using csvstack.\n",
    "\n",
    "2. Which 10 Capital Bikeshare stations were the most popular departing stations in July and August 2018? Which 10 stations were the most popular destination stations in July and August 2018? For both answers (and for the following questions as well), provide the full name, not just the station number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:55:59--  https://s3.amazonaws.com/2018-dmfa/project-1/2018-capitalbikeshare-tripdata.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.189\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.129.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18365732 (18M) [application/zip]\n",
      "Saving to: ‘2018-capitalbikeshare-tripdata.zip’\n",
      "\n",
      "2018-capitalbikesha 100%[===================>]  17.51M   106MB/s    in 0.2s    \n",
      "\n",
      "2018-09-24 01:55:59 (106 MB/s) - ‘2018-capitalbikeshare-tripdata.zip’ saved [18365732/18365732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/project-1/2018-capitalbikeshare-tripdata.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2018-capitalbikeshare-tripdata.zip\n",
      "  inflating: 201807-capitalbikeshare-tripdata.csv  \n",
      "  inflating: 201808-capitalbikeshare-tripdata.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip 2018-capitalbikeshare-tripdata.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine two csv using csvstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!csvstack 201807-capitalbikeshare-tripdata.csv 201808-capitalbikeshare-tripdata.csv > combined.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find out the most popular 10 departing stations in July and August 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   15474 31258 | Lincoln Memorial                                      |\n",
      "| ------------- | ----------------------------------------------------- |\n",
      "|   13082 31623 | Columbus Circle / Union Station                       |\n",
      "|   11999 31247 | Jefferson Dr & 14th St SW                             |\n",
      "|   11671 31289 | Henry Bacon Dr & Lincoln Memorial Circle NW           |\n",
      "|   11650 31288 | 4th St & Madison Dr NW                                |\n",
      "|   10357 31248 | Smithsonian-National Mall / Jefferson Dr & 12th St SW |\n",
      "|    9554 31249 | Jefferson Memorial                                    |\n",
      "|    8620 31290 | 17th St & Independence Ave SW                         |\n",
      "|    8583 31200 | Massachusetts Ave & Dupont Circle NW                  |\n",
      "|    8569 31201 | 15th & P St NW                                        |\n"
     ]
    }
   ],
   "source": [
    "!csvcut combined.csv -c4,5 | sort | uniq -c | sort -rn | head -10 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find out the most popular 10 destination stations in July and August 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   15642 31258 | Lincoln Memorial                                      |\n",
      "| ------------- | ----------------------------------------------------- |\n",
      "|   13635 31623 | Columbus Circle / Union Station                       |\n",
      "|   12135 31247 | Jefferson Dr & 14th St SW                             |\n",
      "|   11722 31289 | Henry Bacon Dr & Lincoln Memorial Circle NW           |\n",
      "|   11555 31288 | 4th St & Madison Dr NW                                |\n",
      "|   10693 31248 | Smithsonian-National Mall / Jefferson Dr & 12th St SW |\n",
      "|    9866 31249 | Jefferson Memorial                                    |\n",
      "|    9141 31200 | Massachusetts Ave & Dupont Circle NW                  |\n",
      "|    8884 31201 | 15th & P St NW                                        |\n",
      "|    8640 31290 | 17th St & Independence Ave SW                         |\n"
     ]
    }
   ],
   "source": [
    "!csvcut combined.csv -c6,7 | sort | uniq -c | sort -rn | head -10 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B (20 points)\n",
    "\n",
    "1. For the most popular departure station, which 10 bikes were used most in trips departing from there?\n",
    "\n",
    "2. Which 10 bikes were used most in trips ending at the most popular destination station?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the 10 bikes which were used most in trips departing from the most popular departure station, No.31258 Lincoln Memorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n",
      "|      18 31258 | Lincoln Memorial | W01311 |\n",
      "| ------------- | ---------------- | ------ |\n",
      "|      17 31258 | Lincoln Memorial | W22553 |\n",
      "|      16 31258 | Lincoln Memorial | W00919 |\n",
      "|      15 31258 | Lincoln Memorial | W23074 |\n",
      "|      15 31258 | Lincoln Memorial | W22567 |\n",
      "|      15 31258 | Lincoln Memorial | W21882 |\n",
      "|      15 31258 | Lincoln Memorial | W20527 |\n",
      "|      14 31258 | Lincoln Memorial | W23311 |\n",
      "|      14 31258 | Lincoln Memorial | W23003 |\n",
      "|      14 31258 | Lincoln Memorial | W22369 |\n"
     ]
    }
   ],
   "source": [
    "!csvcut combined.csv -c4,5,8 | csvgrep -c1 -r \"31258\" | sort | uniq -c | sort -rn | head -10 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the 10 bikes which were used most in trips ending at the most popular destination station, No.31258 Lincoln Memorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n",
      "|      17 31258 | Lincoln Memorial | W22553 |\n",
      "| ------------- | ---------------- | ------ |\n",
      "|      17 31258 | Lincoln Memorial | W01311 |\n",
      "|      16 31258 | Lincoln Memorial | W21882 |\n",
      "|      16 31258 | Lincoln Memorial | W20527 |\n",
      "|      16 31258 | Lincoln Memorial | W00919 |\n",
      "|      15 31258 | Lincoln Memorial | W22567 |\n",
      "|      15 31258 | Lincoln Memorial | W22369 |\n",
      "|      14 31258 | Lincoln Memorial | W23311 |\n",
      "|      14 31258 | Lincoln Memorial | W23074 |\n",
      "|      14 31258 | Lincoln Memorial | W23003 |\n"
     ]
    }
   ],
   "source": [
    "!csvcut combined.csv -c6,7,8 | csvgrep -c1 -r \"31258\" | sort | uniq -c | sort -rn | head -10 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Filters (20 points)\n",
    "\n",
    "In class lectures, previous exercises, and Problems 1 and 2 above, you use Unix commands like grep and tr as filters, changing the text lines streaming through the pipeline. In this problem, write small Python or R programs that act as a filter in the same way, where each program serves one filtering purpose. Name your new filter programs clearly.\n",
    "\n",
    "For this problem, use the Python filter template shown in class and available at https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.py \n",
    "\n",
    "or the R filter template available at https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.R as the basis of your own filters.\n",
    "\n",
    "## Part A (10 points)\n",
    "\n",
    "1. Demonstrate a pipeline that performs a count of the top ten most common words in The Hound of the Baskervilles. This may be exactly the same pipeline we have used before.\n",
    "\n",
    "2. Write your own lowercase filter that replaces tr [:upper:] [:lower:] to transform text into lower case.\n",
    "\n",
    "3. With your new filter, repeat the original pipeline, and substitute your new filters as appropriate. You should obtain the same results as in 3.A.1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:57:15--  https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.py\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.20.245\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.20.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 254 [text/x-python-script]\n",
      "Saving to: ‘simplefilter.py’\n",
      "\n",
      "simplefilter.py     100%[===================>]     254  --.-KB/s    in 0s      \n",
      "\n",
      "2018-09-24 01:57:15 (9.20 MB/s) - ‘simplefilter.py’ saved [254/254]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get R file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:57:18--  https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.R\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.166.5\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.166.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 244 [binary/octet-stream]\n",
      "Saving to: ‘simplefilter.R’\n",
      "\n",
      "simplefilter.R      100%[===================>]     244  --.-KB/s    in 0s      \n",
      "\n",
      "2018-09-24 01:57:19 (12.2 MB/s) - ‘simplefilter.R’ saved [244/244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/project-1/simplefilter.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count of the top ten most common words in The Hound of the Baskervilles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3515 the\n",
      "   1716 of\n",
      "   1696 and\n",
      "   1501 i\n",
      "   1488 to\n",
      "   1369 a\n",
      "   1159 that\n",
      "   1025 it\n",
      "    968 in\n",
      "    922 he\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat hound.txt | tr -sc '[:alpha:]' '[\\n*]' | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the question 2 and 3 using Python codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your own lowercase filter that replaces tr [:upper:] [:lower:] to transform text into lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the Python file as below and rename it as lower.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "A filter that _________.\n",
    "\"\"\"\n",
    "\n",
    "import fileinput\n",
    "\n",
    "\n",
    "def process(line):\n",
    "    \"\"\"For each line of input, _____.\"\"\"\n",
    "    line = line[:-1]\n",
    "    # Do something more here!\n",
    "    print(line.lower())\n",
    "\n",
    "\n",
    "for line in fileinput.input():\n",
    "    process(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x lower.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have a try to apply lowercase filter to hound.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project gutenberg’s the hound of the baskervilles, by a. conan doyle\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\n",
      "re-use it under the terms of the project gutenberg license included\n",
      "with this ebook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "title: the hound of the baskervilles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head hound.txt | ./lower.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With your new filter, repeat the original pipeline, and substitute your new filters as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3515 the\n",
      "   1716 of\n",
      "   1696 and\n",
      "   1501 i\n",
      "   1488 to\n",
      "   1369 a\n",
      "   1159 that\n",
      "   1025 it\n",
      "    968 in\n",
      "    922 he\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat hound.txt | tr -sc '[:alpha:]' '[\\n*]' | ./lower.py | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the question 2 and 3 using R codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your own lowercase filter that replaces tr [:upper:] [:lower:] to transform text into lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the R file as below and rename it as lower.R"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env Rscript\n",
    "\n",
    "process <- function (input) {\n",
    "  # Do something more here!\n",
    "  output <- tolower(input)\n",
    "  return(output)\n",
    "}\n",
    "\n",
    "\n",
    "f <- file(\"stdin\")\n",
    "open(f)\n",
    "while(length(line <- readLines(f, n=1)) > 0) {\n",
    "  line = process(line)\n",
    "  write(line, stdout())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x lower.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have a try to apply lowercase filter to hound.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project gutenberg’s the hound of the baskervilles, by a. conan doyle\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\n",
      "re-use it under the terms of the project gutenberg license included\n",
      "with this ebook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "title: the hound of the baskervilles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head hound.txt | ./lower.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With your new filter, repeat the original pipeline, and substitute your new filters as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3515 the\n",
      "   1716 of\n",
      "   1696 and\n",
      "   1501 i\n",
      "   1488 to\n",
      "   1369 a\n",
      "   1159 that\n",
      "   1025 it\n",
      "    968 in\n",
      "    922 he\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat hound.txt | tr -sc '[:alpha:]' '[\\n*]' | ./lower.R | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B (10 points)\n",
    "\n",
    "1. Write a filter that removes at least ten common words of English text, commonly known as “stop words”. Sources of English stop word lists are readily available online, or you may generate your own list from the text. Document your process for obtaining or determining this list.\n",
    "\n",
    "2. Add your stop word filter to a word count pipeline and show the most common 25 words in The Hound of the Baskervilles when stop words are removed. You may re-use your filter from Part A if you wish, although this is not required for full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We find a txt file link from the Wikipedia page of 'Stop words'. Get stop-word-list.txt file from the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-24 01:57:56--  http://xpo6.com/wp-content/uploads/2015/01/stop-word-list.txt\n",
      "Resolving xpo6.com (xpo6.com)... 104.31.84.205, 104.31.85.205\n",
      "Connecting to xpo6.com (xpo6.com)|104.31.84.205|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2231 (2.2K) [text/plain]\n",
      "Saving to: ‘stop-word-list.txt’\n",
      "\n",
      "stop-word-list.txt  100%[===================>]   2.18K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-09-24 01:57:57 (11.3 MB/s) - ‘stop-word-list.txt’ saved [2231/2231]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://xpo6.com/wp-content/uploads/2015/01/stop-word-list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem using Python codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the Python file as below and rename it as stop_words_remove.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "A filter that _________.\n",
    "\"\"\"\n",
    "\n",
    "import fileinput\n",
    "\n",
    "def process(line):\n",
    "    \"\"\"For each line of input, _____.\"\"\"\n",
    "    line = line[:-1]\n",
    "\n",
    "    # read the stop-word-list.txt file\n",
    "    file = open(\"stop-word-list.txt\", \"r\")\n",
    "    stopwords = file.read()\n",
    "    \n",
    "    # replace each stop words with None\n",
    "    for word in stopwords.split(\"\\n\"):\n",
    "        if word == line:\n",
    "            line = None\n",
    "            \n",
    "    print(line)\n",
    "\n",
    "for line in fileinput.input():\n",
    "    process(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x stop_words_remove.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the pipeline: show all contents | spilit into one words each line | run the lowercase filter | run the removing stop words filter | get rid of the lines of 'None' | sort and count words\n",
    "\n",
    "##### Because in the Python file we replace the stop words as 'None', in our pipeline we add a filter to get rid of the lines of 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    350 sir\n",
      "    247 s\n",
      "    240 said\n",
      "    213 man\n",
      "    193 holmes\n",
      "    163 moor\n",
      "    156 henry\n",
      "    119 know\n",
      "    114 watson\n",
      "    114 baskerville\n",
      "    113 did\n",
      "    109 dr\n",
      "     94 charles\n",
      "     93 stapleton\n",
      "     93 gutenberg\n",
      "     90 think\n",
      "     90 mortimer\n",
      "     89 come\n",
      "     87 project\n",
      "     86 time\n",
      "     77 face\n",
      "     76 hound\n",
      "     74 came\n",
      "     73 night\n",
      "     73 house\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat hound.txt | tr -sc '[:alpha:]' '[\\n*]' | ./lower.py | ./stop_words_remove.py | grep -v 'None' | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem using R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the R file as below and rename it as stop_words_remove.R"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env Rscript\n",
    "\n",
    "stopwords <- read.table(\"stop-word-list.txt\")\n",
    "\n",
    "process <- function (input) {\n",
    "  # Do something more here!\n",
    "  if (input %in% stopwords[,1] == FALSE) {\n",
    "    output <- input\n",
    "  }\n",
    "    else {output <- NULL}\n",
    "  return(output)\n",
    "}\n",
    "\n",
    "\n",
    "f <- file(\"stdin\")\n",
    "open(f)\n",
    "while(length(line <- readLines(f, n=1)) > 0) {\n",
    "  line = process(line)\n",
    "  write(line, stdout())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x stop_words_remove.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the pipeline: show all contents | spilit into one words each line | run the lowercase filter | run the removing stop words filter | only get the lines containing word | sort and count words\n",
    "\n",
    "##### Because in the R file we replace the stop words as 'NULL', in our pipeline we add a filter to get only the lines containing word rather than 'NULL'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    350 sir\n",
      "    247 s\n",
      "    240 said\n",
      "    213 man\n",
      "    193 holmes\n",
      "    163 moor\n",
      "    156 henry\n",
      "    119 know\n",
      "    114 watson\n",
      "    114 baskerville\n",
      "    113 did\n",
      "    109 dr\n",
      "     94 charles\n",
      "     93 stapleton\n",
      "     93 gutenberg\n",
      "     90 think\n",
      "     90 mortimer\n",
      "     89 come\n",
      "     87 project\n",
      "     86 time\n",
      "     77 face\n",
      "     76 hound\n",
      "     74 came\n",
      "     73 night\n",
      "     73 house\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat hound.txt | tr -sc '[:alpha:]' '[\\n*]' | ./lower.R | ./stop_words_remove.R | grep '[[:print:]]' | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
